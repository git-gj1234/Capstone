{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2885538c",
   "metadata": {},
   "source": [
    "⚙️ Config-Driven Legal RAG Indexer with LanceDB + Advanced Chunking (Unstructured & Langchain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports-modified",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Github\\Capstone\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import fitz  # PyMuPDF (Ensure PyMuPDF is installed: pip install PyMuPDF)\n",
    "import lancedb\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional\n",
    "from lancedb.pydantic import Vector, LanceModel\n",
    "import csv\n",
    "\n",
    "# New imports for advanced processing\n",
    "import torch\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "# MODIFIED IMPORT for HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings # pip install langchain-huggingface\n",
    "try:\n",
    "    from langchain_experimental.text_splitter import SemanticChunker\n",
    "except ImportError:\n",
    "    from langchain.text_splitter import SemanticChunker # If it has moved to core langchain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Your custom embedder (ensure legalbert_embedder.py is present)\n",
    "from legalbert_embedder import LegalBERTEmbedder\n",
    "# Optionally, for using other sentence transformers as main_embedder\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 CONFIGURATION\n",
    "config = {\n",
    "    \"model_name\": \"nlpaueb/legal-bert-base-uncased\", # Used by LegalBERTEmbedder if not overridden\n",
    "    \"semantic_chunker_model\": \"thenlper/gte-base\", # Model for SemanticChunker\n",
    "    \"comp_pdf\": \"CompaniesAct.pdf\",\n",
    "    \"bank_pdf\": \"BankruptcyAct.pdf\",\n",
    "    \"db_path\": \"./Data\",\n",
    "\n",
    "    # Table names for LanceDB\n",
    "    \"comp_table\": \"CompaniesAct\",\n",
    "    \"bank_table\": \"BankruptcyAct\",\n",
    "    \"constitution_table\": \"IndianConstitution\",\n",
    "\n",
    "    # CSV path for Indian Constitution\n",
    "    \"constitution_csv\": \"Indian_Constitution.csv\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load-pdf-fitz",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_text(path: str) -> List[str]:\n",
    "    doc = fitz.open(path)\n",
    "    return [page.get_text() for page in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "extract-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sections_bankruptcy_act(full_text: str, config: Dict) -> List[Dict]:\n",
    "    section_pattern = re.compile(r\"(?i)(SECTION\\s*\\d+[A-Z]?(?:\\.\\d+)?(?:[A-Z]*)?)\")\n",
    "    part_pattern = re.compile(r\"(?i)^\\s*(PART\\s+[A-Z]+.*?)$\", re.MULTILINE)\n",
    "    chapter_pattern = re.compile(r\"(?i)^\\s*(CHAPTER\\s+[IVXLC]+.*?)$\", re.MULTILINE)\n",
    "\n",
    "    # Find headings\n",
    "    section_matches = list(section_pattern.finditer(full_text))\n",
    "    part_matches = list(part_pattern.finditer(full_text))\n",
    "    chapter_matches = list(chapter_pattern.finditer(full_text))\n",
    "\n",
    "    # Mapping start locations\n",
    "    part_map = {m.start(): m.group(1).strip() for m in part_matches}\n",
    "    chapter_map = {m.start(): m.group(1).strip() for m in chapter_matches}\n",
    "    part_starts = sorted(part_map.keys())\n",
    "    chapter_starts = sorted(chapter_map.keys())\n",
    "\n",
    "    chunks = []\n",
    "    for i, match in enumerate(section_matches):\n",
    "        start = match.start()\n",
    "        end = section_matches[i+1].start() if i+1 < len(section_matches) else len(full_text)\n",
    "        chunk_text = full_text[start:end].strip()\n",
    "        section_title = match.group(1).strip()\n",
    "\n",
    "        # Find closest PART\n",
    "        part_title = None\n",
    "        for p_start in reversed(part_starts):\n",
    "            if p_start <= start:\n",
    "                part_title = part_map[p_start]\n",
    "                break\n",
    "\n",
    "        # Find closest CHAPTER\n",
    "        chapter_title = None\n",
    "        for ch_start in reversed(chapter_starts):\n",
    "            if ch_start <= start:\n",
    "                chapter_title = chapter_map[ch_start]\n",
    "                break\n",
    "\n",
    "        chunks.append({\n",
    "            \"id\": f\"bankruptcy_section_{i}\",\n",
    "            \"chunk\": chunk_text,\n",
    "            \"section_title\": section_title,\n",
    "            \"chapter_title\": chapter_title,\n",
    "            \"part_title\": part_title,\n",
    "            \"page\": None, # Page info not reliably extracted by regex from full text\n",
    "            \"source\": \"Bankruptcy Act\",\n",
    "        })\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "extract-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sections_with_meta_comp(full_text: str, config: Dict) -> List[Dict]:\n",
    "    section_pattern = re.compile(r\"(?i)(SECTION\\s*\\d+[A-Z]?(?:\\.\\d+)?[A-Z]*)\")\n",
    "    chapter_pattern = re.compile(r\"(?i)^\\s*(CHAPTER\\s+[IVXLC]+.*?)$\", re.MULTILINE)\n",
    "\n",
    "    matches = list(section_pattern.finditer(full_text))\n",
    "    chapter_matches = list(chapter_pattern.finditer(full_text))\n",
    "\n",
    "    # Map chapter start positions to titles\n",
    "    chapter_map = {m.start(): m.group(1).strip() for m in chapter_matches}\n",
    "    chapter_starts = sorted(chapter_map.keys())\n",
    "\n",
    "    chunks = []\n",
    "    for i, match in enumerate(matches):\n",
    "        start = match.start()\n",
    "        end = matches[i + 1].start() if i + 1 < len(matches) else len(full_text)\n",
    "        chunk_text = full_text[start:end].strip()\n",
    "        section_title = match.group(1).strip()\n",
    "\n",
    "        # Get closest preceding chapter\n",
    "        chapter_title = None\n",
    "        for ch_start in reversed(chapter_starts):\n",
    "            if ch_start <= start:\n",
    "                chapter_title = chapter_map[ch_start]\n",
    "                break\n",
    "\n",
    "        chunks.append({\n",
    "            \"id\": f\"section_{i}\",\n",
    "            \"chunk\": chunk_text,\n",
    "            \"section_title\": section_title,\n",
    "            \"chapter_title\": chapter_title,\n",
    "            \"page\": None,\n",
    "            \"source\": \"Companies Act\",\n",
    "        })\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "lancedb-constitution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lancedb_index_constitution(chunks: List[Dict], embeddings: np.ndarray, db_path: str, table_name: str):\n",
    "    # Dynamically get embedding dimension from the provided embeddings\n",
    "    if embeddings.ndim == 1: # Single embedding\n",
    "        embedding_dim = embeddings.shape[0]\n",
    "    elif embeddings.ndim == 2: # Array of embeddings\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected embeddings shape: {embeddings.shape}. Expected 1D or 2D array.\")\n",
    "    print(f\"   LanceDB: Using embedding dimension {embedding_dim} for table {table_name}\")\n",
    "\n",
    "    class ConstitutionArticle(LanceModel):\n",
    "        id: str\n",
    "        chunk: str\n",
    "        embedding: Vector(embedding_dim)\n",
    "        section_title: str # Assuming article_id from CSV is always a string\n",
    "        chapter_title: str # Assuming this is always \"Indian Constitution\"\n",
    "        page: Optional[int] = None # Page can be None\n",
    "        source: str\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(db_path):\n",
    "            os.makedirs(db_path)\n",
    "        db = lancedb.connect(db_path)\n",
    "\n",
    "        data_to_insert = []\n",
    "        for i in range(len(chunks)):\n",
    "            item = {\n",
    "                \"id\": chunks[i].get(\"id\", f\"const_default_id_{i}\"),\n",
    "                \"chunk\": chunks[i].get(\"chunk\", \"\"),\n",
    "                \"embedding\": embeddings[i].tolist(),\n",
    "                \"section_title\": chunks[i].get(\"section_title\", \"Unknown Article\"),\n",
    "                \"chapter_title\": chunks[i].get(\"chapter_title\", \"Indian Constitution\"),\n",
    "                \"page\": chunks[i].get(\"page\"),\n",
    "                \"source\": chunks[i].get(\"source\", \"Indian Constitution\")\n",
    "            }\n",
    "            data_to_insert.append(item)\n",
    "\n",
    "        if table_name in db.table_names():\n",
    "            db.drop_table(table_name)\n",
    "            print(f\"Dropped existing table: {table_name}\")\n",
    "\n",
    "        table = db.create_table(table_name, schema=ConstitutionArticle)\n",
    "        if data_to_insert:\n",
    "            table.add(data_to_insert)\n",
    "            print(f\"Successfully added {len(data_to_insert)} records to {table_name}\")\n",
    "        else:\n",
    "            print(f\"No data to insert into {table_name}\")\n",
    "        return table\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating LanceDB index for {table_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "lancedb-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lancedb_index_bankruptcy(chunks: List[Dict], embeddings: np.ndarray, db_path: str, table_name: str):\n",
    "    # Dynamically get embedding dimension from the provided embeddings\n",
    "    if embeddings.ndim == 1: # Single embedding\n",
    "        embedding_dim = embeddings.shape[0]\n",
    "    elif embeddings.ndim == 2: # Array of embeddings\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected embeddings shape: {embeddings.shape}. Expected 1D or 2D array.\")\n",
    "    print(f\"   LanceDB: Using embedding dimension {embedding_dim} for table {table_name}\")\n",
    "    \n",
    "    class Document1(LanceModel):\n",
    "        id: str\n",
    "        chunk: str\n",
    "        embedding: Vector(embedding_dim)\n",
    "        part_title: Optional[str] = None\n",
    "        chapter_title: Optional[str] = None\n",
    "        section_title: Optional[str] = None\n",
    "        page: Optional[int] = None\n",
    "        source: str\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(db_path): os.makedirs(db_path)\n",
    "        db = lancedb.connect(db_path)\n",
    "        data_to_insert = []\n",
    "        for i in range(len(chunks)):\n",
    "            item = {\n",
    "                \"id\": chunks[i].get(\"id\", f\"bank_default_id_{i}\"),\n",
    "                \"chunk\": chunks[i].get(\"chunk\", \"\"),\n",
    "                \"embedding\": embeddings[i].tolist(),\n",
    "                \"part_title\": chunks[i].get(\"part_title\"), # .get() correctly returns None if key missing\n",
    "                \"chapter_title\": chunks[i].get(\"chapter_title\"),\n",
    "                \"section_title\": chunks[i].get(\"section_title\"),\n",
    "                \"page\": chunks[i].get(\"page\"),\n",
    "                \"source\": chunks[i].get(\"source\", \"Bankruptcy Act\")\n",
    "            }\n",
    "            data_to_insert.append(item)\n",
    "\n",
    "        if table_name in db.table_names():\n",
    "            db.drop_table(table_name)\n",
    "            print(f\"Dropped existing table: {table_name}\")\n",
    "\n",
    "        table = db.create_table(table_name, schema=Document1)\n",
    "        if data_to_insert:\n",
    "            table.add(data_to_insert)\n",
    "            print(f\"Successfully added {len(data_to_insert)} records to {table_name}\")\n",
    "        else:\n",
    "            print(f\"No data to insert into {table_name}\")\n",
    "        return table\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating LanceDB index for {table_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "lancedb-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lancedb_index_company(chunks: List[Dict], embeddings: np.ndarray, db_path: str, table_name: str):\n",
    "    # Dynamically get embedding dimension from the provided embeddings\n",
    "    if embeddings.ndim == 1: # Single embedding\n",
    "        embedding_dim = embeddings.shape[0]\n",
    "    elif embeddings.ndim == 2: # Array of embeddings\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected embeddings shape: {embeddings.shape}. Expected 1D or 2D array.\")\n",
    "    print(f\"   LanceDB: Using embedding dimension {embedding_dim} for table {table_name}\")\n",
    "    \n",
    "    class Document2(LanceModel):\n",
    "        id: str\n",
    "        chunk: str\n",
    "        embedding: Vector(embedding_dim)\n",
    "        section_title: Optional[str] = None\n",
    "        chapter_title: Optional[str] = None\n",
    "        page: Optional[int] = None\n",
    "        source: str\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(db_path): os.makedirs(db_path)\n",
    "        db = lancedb.connect(db_path)\n",
    "        data_to_insert = []\n",
    "        for i in range(len(chunks)):\n",
    "            item = {\n",
    "                \"id\": chunks[i].get(\"id\", f\"comp_default_id_{i}\"),\n",
    "                \"chunk\": chunks[i].get(\"chunk\", \"\"),\n",
    "                \"embedding\": embeddings[i].tolist(),\n",
    "                \"section_title\": chunks[i].get(\"section_title\"),\n",
    "                \"chapter_title\": chunks[i].get(\"chapter_title\"),\n",
    "                \"page\": chunks[i].get(\"page\"),\n",
    "                \"source\": chunks[i].get(\"source\", \"Companies Act\")\n",
    "            }\n",
    "            data_to_insert.append(item)\n",
    "\n",
    "        if table_name in db.table_names():\n",
    "            db.drop_table(table_name)\n",
    "            print(f\"Dropped existing table: {table_name}\")\n",
    "\n",
    "        table = db.create_table(table_name, schema=Document2)\n",
    "        if data_to_insert:\n",
    "            table.add(data_to_insert)\n",
    "            print(f\"Successfully added {len(data_to_insert)} records to {table_name}\")\n",
    "        else:\n",
    "            print(f\"No data to insert into {table_name}\")\n",
    "        return table\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating LanceDB index for {table_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "load-constitution-csv",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_constitution_articles(file_path: str) -> List[Dict]:\n",
    "    articles = []\n",
    "    try:\n",
    "        with open(file_path, newline='', encoding='utf-8') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for i, row in enumerate(reader):\n",
    "                articles.append({\n",
    "                    \"id\": f\"constitution_article_{i}\",\n",
    "                    \"chunk\": row.get(\"article_desc\", \"\").strip(),\n",
    "                    \"section_title\": row.get(\"article_id\", f\"UnknownArticle_{i}\").strip(), # Corresponds to article_id\n",
    "                    \"chapter_title\": \"Indian Constitution\", # Assigning a general chapter title\n",
    "                    \"page\": None,\n",
    "                    \"source\": \"Indian Constitution\"\n",
    "                })\n",
    "        return articles\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ CSV file not found: {file_path}\")\n",
    "    except KeyError as ke:\n",
    "        print(f\"❌ Missing column in CSV (expected 'article_desc', 'article_id'): {ke}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading Constitution articles: {e}\")\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "get-semantic-embedder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Global Instance for Semantic Chunker Embeddings ---\n",
    "_semantic_embeddings_instance = None\n",
    "\n",
    "def get_semantic_embeddings_instance(model_name=\"thenlper/gte-base\"):\n",
    "    global _semantic_embeddings_instance\n",
    "    if _semantic_embeddings_instance is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"ℹ️ Initializing Semantic Embeddings on '{device}' with model '{model_name}' for chunking.\")\n",
    "        # This will now use the new import path from langchain_huggingface\n",
    "        _semantic_embeddings_instance = HuggingFaceEmbeddings(\n",
    "            model_name=model_name,\n",
    "            model_kwargs={'device': device},\n",
    "            encode_kwargs={'normalize_embeddings': True} # GTE models often benefit from normalization\n",
    "        )\n",
    "    return _semantic_embeddings_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "process-pdf-advanced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_document_advanced(\n",
    "    pdf_path: str,\n",
    "    config: Dict,\n",
    "    document_type: str, # e.g., \"CompaniesAct\", \"BankruptcyAct\"\n",
    "    semantic_chunker_embeddings # Pass the initialized embeddings instance\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Processes a PDF document using Unstructured for parsing, regex for coarse structuring,\n",
    "    and SemanticChunker/RecursiveCharacterTextSplitter for fine-grained chunking.\n",
    "    \"\"\"\n",
    "    print(f\"📄 Advanced processing for: {pdf_path}\")\n",
    "    final_processed_chunks = []\n",
    "    full_text = \"\"\n",
    "\n",
    "    try:\n",
    "        # 1. Use Unstructured to Parse the PDF\n",
    "        print(f\"    Parsing with Unstructured (strategy: hi_res)... This may take a while.\")\n",
    "        print(\"    Ensure Poppler is installed and in PATH for 'hi_res' PDF strategy if not using OCR-only methods.\")\n",
    "        elements = partition_pdf(\n",
    "            filename=pdf_path,\n",
    "            strategy=\"hi_res\", # Uses layout detection. Requires Poppler and Detectron2.\n",
    "                               # Fallback to \"fast\" or \"ocr_only\" (needs Tesseract) if \"hi_res\" has issues.\n",
    "            infer_table_structure=True,\n",
    "            # model_name=\"yolox\" # Default for hi_res, usually not needed to specify\n",
    "        )\n",
    "        full_text = \"\\n\\n\".join([el.text for el in elements if hasattr(el, 'text') and el.text.strip()])\n",
    "        if not full_text.strip():\n",
    "            print(f\"⚠️ Unstructured extracted no text from {pdf_path}.\")\n",
    "            # Proceed to Fitz fallback if Unstructured returns nothing, even if no explicit error.\n",
    "            raise ValueError(\"Unstructured returned no text.\")\n",
    "        print(f\"    Unstructured extracted text length: {len(full_text)} chars.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during Unstructured parsing for {pdf_path}: {e}\")\n",
    "        print(f\"    Falling back to Fitz (PyMuPDF) for PDF text extraction.\")\n",
    "        try:\n",
    "            # Ensure PyMuPDF is installed: pip install PyMuPDF\n",
    "            doc = fitz.open(pdf_path) # This is the correct way to open with PyMuPDF\n",
    "            page_texts = [page.get_text() for page in doc]\n",
    "            doc.close()\n",
    "            full_text = \"\\n\".join(page_texts)\n",
    "            if not full_text.strip():\n",
    "                print(f\"⚠️ Fitz (PyMuPDF) also extracted no text from {pdf_path}.\")\n",
    "                return [] # Critical failure if both methods yield no text\n",
    "            print(f\"    Fitz (PyMuPDF) extracted text length: {len(full_text)} chars.\")\n",
    "        except Exception as e_fitz:\n",
    "            print(f\"❌ Fitz (PyMuPDF) also failed for {pdf_path}: {e_fitz}\")\n",
    "            return [] # Critical failure\n",
    "\n",
    "    # ... (rest of the function remains the same as it uses full_text) ...\n",
    "\n",
    "    # 2. Use existing regex for Coarse Chunking (Sections, Chapters, Parts)\n",
    "    print(f\"    Applying regex for coarse structuring...\")\n",
    "    coarse_chunks_with_meta = []\n",
    "    if document_type == \"BankruptcyAct\":\n",
    "        coarse_chunks_with_meta = extract_sections_bankruptcy_act(full_text, config)\n",
    "    elif document_type == \"CompaniesAct\":\n",
    "        coarse_chunks_with_meta = extract_sections_with_meta_comp(full_text, config)\n",
    "    else:\n",
    "        print(f\"⚠️ Unknown document type for regex extraction: {document_type}\")\n",
    "        coarse_chunks_with_meta = [{\"id\": \"doc_0\", \"chunk\": full_text, \"section_title\": \"Full Document\", \"source\": document_type}]\n",
    "\n",
    "\n",
    "    if not coarse_chunks_with_meta:\n",
    "        print(f\"⚠️ Regex extraction yielded no coarse chunks for {pdf_path}.\")\n",
    "        return []\n",
    "    print(f\"    Found {len(coarse_chunks_with_meta)} coarse chunks via regex.\")\n",
    "\n",
    "    # 3. Fine-Grained Chunking for long coarse chunks\n",
    "    print(f\"    Fine-graining long chunks...\")\n",
    "    semantic_text_splitter = SemanticChunker(\n",
    "        semantic_chunker_embeddings,\n",
    "        breakpoint_threshold_type=\"percentile\"\n",
    "    )\n",
    "    recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=160,\n",
    "        separators=[\"\\n\\nSECTION \", \"\\n\\nPART \", \"\\n\\nCHAPTER \", \"\\n\\n\", \"\\n\", \". \", \", \", \" \", \"\"],\n",
    "        length_function=len\n",
    "    )\n",
    "\n",
    "    LENGTH_THRESHOLD_CHARS = 1500\n",
    "    MAX_CHARS_FOR_SEMANTIC_CHUNK_INPUT = 25000 # Safety for SemanticChunker\n",
    "\n",
    "    for i, coarse_chunk in enumerate(coarse_chunks_with_meta):\n",
    "        section_text = coarse_chunk[\"chunk\"]\n",
    "\n",
    "        if len(section_text) > LENGTH_THRESHOLD_CHARS:\n",
    "            sub_chunks_texts = []\n",
    "            try:\n",
    "                if len(section_text) < MAX_CHARS_FOR_SEMANTIC_CHUNK_INPUT:\n",
    "                    # print(f\"      Semantically splitting coarse chunk {coarse_chunk.get('id',i)} ({len(section_text)} chars)\")\n",
    "                    sub_chunks_texts = semantic_text_splitter.split_text(section_text)\n",
    "                else:\n",
    "                    # print(f\"      Coarse chunk {coarse_chunk.get('id',i)} too long for SemanticChunker ({len(section_text)} chars). Using Recursive.\")\n",
    "                    sub_chunks_texts = recursive_splitter.split_text(section_text)\n",
    "            except Exception as e_semantic:\n",
    "                # print(f\"      ⚠️ SemanticChunker failed for coarse chunk {coarse_chunk.get('id', i)}: {e_semantic}. Falling back to Recursive.\")\n",
    "                sub_chunks_texts = recursive_splitter.split_text(section_text)\n",
    "\n",
    "            for j, sub_chunk_text in enumerate(sub_chunks_texts):\n",
    "                if not sub_chunk_text.strip(): continue\n",
    "                final_processed_chunks.append({\n",
    "                    \"id\": f\"{coarse_chunk.get('id', f'c_{i}')}_sub_{j}\",\n",
    "                    \"chunk\": sub_chunk_text.strip(),\n",
    "                    \"section_title\": coarse_chunk.get(\"section_title\"),\n",
    "                    \"chapter_title\": coarse_chunk.get(\"chapter_title\"),\n",
    "                    \"part_title\": coarse_chunk.get(\"part_title\"),\n",
    "                    \"page\": coarse_chunk.get(\"page\"),\n",
    "                    \"source\": coarse_chunk.get(\"source\"),\n",
    "                })\n",
    "        elif section_text.strip():\n",
    "            coarse_chunk[\"chunk\"] = coarse_chunk[\"chunk\"].strip()\n",
    "            final_processed_chunks.append(coarse_chunk)\n",
    "\n",
    "    print(f\"    Processed into {len(final_processed_chunks)} final chunks for {pdf_path}.\")\n",
    "    return final_processed_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "index-bankruptcy-advanced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_bankruptcy_act_advanced(config: Dict, main_embedder_instance) -> None:\n",
    "    print(\"\\n⚙️ Indexing Bankruptcy Act (Advanced Pipeline)...\")\n",
    "    semantic_embeddings = get_semantic_embeddings_instance(config.get(\"semantic_chunker_model\", \"thenlper/gte-base\"))\n",
    "    processed_chunks = process_pdf_document_advanced(\n",
    "        pdf_path=config[\"bank_pdf\"],\n",
    "        config=config,\n",
    "        document_type=\"BankruptcyAct\",\n",
    "        semantic_chunker_embeddings=semantic_embeddings\n",
    "    )\n",
    "    if not processed_chunks:\n",
    "        print(\"⚠️ No chunks to index for Bankruptcy Act.\")\n",
    "        return\n",
    "\n",
    "    print(f\"    Embedding {len(processed_chunks)} chunks with main embedder...\")\n",
    "    embeddings_np = main_embedder_instance.encode([c[\"chunk\"] for c in processed_chunks])\n",
    "    table_created = create_lancedb_index_bankruptcy(processed_chunks, embeddings_np, config[\"db_path\"], config[\"bank_table\"])\n",
    "    if table_created: # Check if table creation was successful\n",
    "        print(f\"✅ LanceDB index created/updated for '{config['bank_table']}'.\")\n",
    "    else:\n",
    "        print(f\"⚠️ LanceDB index creation FAILED for '{config['bank_table']}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "index-company-advanced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_company_act_advanced(config: Dict, main_embedder_instance) -> None:\n",
    "    print(\"\\n⚙️ Indexing Company Act (Advanced Pipeline)...\")\n",
    "    semantic_embeddings = get_semantic_embeddings_instance(config.get(\"semantic_chunker_model\", \"thenlper/gte-base\"))\n",
    "    processed_chunks = process_pdf_document_advanced(\n",
    "        pdf_path=config[\"comp_pdf\"],\n",
    "        config=config,\n",
    "        document_type=\"CompaniesAct\",\n",
    "        semantic_chunker_embeddings=semantic_embeddings\n",
    "    )\n",
    "    if not processed_chunks:\n",
    "        print(\"⚠️ No chunks to index for Company Act.\")\n",
    "        return\n",
    "\n",
    "    print(f\"    Embedding {len(processed_chunks)} chunks with main embedder...\")\n",
    "    embeddings_np = main_embedder_instance.encode([c[\"chunk\"] for c in processed_chunks])\n",
    "    table_created = create_lancedb_index_company(processed_chunks, embeddings_np, config[\"db_path\"], config[\"comp_table\"])\n",
    "    if table_created: # Check if table creation was successful\n",
    "        print(f\"✅ LanceDB index created/updated for '{config['comp_table']}'.\")\n",
    "    else:\n",
    "        print(f\"⚠️ LanceDB index creation FAILED for '{config['comp_table']}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "index-constitution-advanced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_constitution_articles_advanced(config: Dict, main_embedder_instance) -> None:\n",
    "    print(\"\\n⚙️ Indexing Indian Constitution from CSV (Advanced Pipeline)...\")\n",
    "    coarse_chunks_with_meta = load_constitution_articles(config[\"constitution_csv\"])\n",
    "    if not coarse_chunks_with_meta:\n",
    "        print(\"⚠️ No articles loaded from Constitution CSV. Skipping indexing.\")\n",
    "        return\n",
    "    print(f\"    Loaded {len(coarse_chunks_with_meta)} articles from CSV.\")\n",
    "\n",
    "    final_processed_chunks = []\n",
    "    semantic_embeddings = get_semantic_embeddings_instance(config.get(\"semantic_chunker_model\", \"thenlper/gte-base\"))\n",
    "    semantic_text_splitter = SemanticChunker(\n",
    "        semantic_embeddings, breakpoint_threshold_type=\"percentile\"\n",
    "    )\n",
    "    recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800, chunk_overlap=160, separators=[\"\\n\\n\", \"\\n\", \". \", \", \", \" \", \"\"], length_function=len\n",
    "    )\n",
    "    LENGTH_THRESHOLD_CHARS = 1500\n",
    "    MAX_CHARS_FOR_SEMANTIC_CHUNK_INPUT = 25000\n",
    "\n",
    "    print(f\"    Fine-graining long articles...\")\n",
    "    for i, coarse_chunk in enumerate(coarse_chunks_with_meta):\n",
    "        article_text = coarse_chunk[\"chunk\"]\n",
    "        if len(article_text) > LENGTH_THRESHOLD_CHARS:\n",
    "            sub_chunks_texts = []\n",
    "            try:\n",
    "                if len(article_text) < MAX_CHARS_FOR_SEMANTIC_CHUNK_INPUT:\n",
    "                    sub_chunks_texts = semantic_text_splitter.split_text(article_text)\n",
    "                else:\n",
    "                    sub_chunks_texts = recursive_splitter.split_text(article_text)\n",
    "            except Exception as e_semantic:\n",
    "                sub_chunks_texts = recursive_splitter.split_text(article_text)\n",
    "\n",
    "            for j, sub_chunk_text in enumerate(sub_chunks_texts):\n",
    "                if not sub_chunk_text.strip(): continue\n",
    "                final_processed_chunks.append({\n",
    "                    \"id\": f\"{coarse_chunk.get('id', f'csv_{i}')}_sub_{j}\",\n",
    "                    \"chunk\": sub_chunk_text.strip(),\n",
    "                    \"section_title\": coarse_chunk.get(\"section_title\"),\n",
    "                    \"chapter_title\": coarse_chunk.get(\"chapter_title\"),\n",
    "                    \"page\": coarse_chunk.get(\"page\"),\n",
    "                    \"source\": coarse_chunk.get(\"source\"),\n",
    "                })\n",
    "        elif article_text.strip():\n",
    "            coarse_chunk[\"chunk\"] = coarse_chunk[\"chunk\"].strip()\n",
    "            final_processed_chunks.append(coarse_chunk)\n",
    "\n",
    "    if not final_processed_chunks:\n",
    "        print(\"⚠️ No final chunks to index for Constitution.\")\n",
    "        return\n",
    "\n",
    "    print(f\"    Processed into {len(final_processed_chunks)} final chunks for Constitution.\")\n",
    "    print(f\"    Embedding {len(final_processed_chunks)} chunks with main embedder...\")\n",
    "    embeddings_np = main_embedder_instance.encode([c[\"chunk\"] for c in final_processed_chunks])\n",
    "    table_created = create_lancedb_index_constitution(final_processed_chunks, embeddings_np, config[\"db_path\"], config[\"constitution_table\"])\n",
    "    if table_created: # Check if table creation was successful\n",
    "        print(f\"✅ LanceDB index created/updated for '{config['constitution_table']}'.\")\n",
    "    else:\n",
    "        print(f\"⚠️ LanceDB index creation FAILED for '{config['constitution_table']}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "run-advanced-pipeline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️ Initializing main embedder for LanceDB storage...\n",
      "   Using 'sentence-transformers/all-mpnet-base-v2' as the main embedder.\n",
      "\n",
      "⚙️ Indexing Bankruptcy Act (Advanced Pipeline)...\n",
      "ℹ️ Initializing Semantic Embeddings on 'cpu' with model 'thenlper/gte-base' for chunking.\n",
      "📄 Advanced processing for: BankruptcyAct.pdf\n",
      "    Parsing with Unstructured (strategy: hi_res)... This may take a while.\n",
      "    Ensure Poppler is installed and in PATH for 'hi_res' PDF strategy if not using OCR-only methods.\n",
      "    Unstructured extracted text length: 385848 chars.\n",
      "    Applying regex for coarse structuring...\n",
      "    Found 507 coarse chunks via regex.\n",
      "    Fine-graining long chunks...\n",
      "    Processed into 597 final chunks for BankruptcyAct.pdf.\n",
      "    Embedding 597 chunks with main embedder...\n",
      "   LanceDB: Using embedding dimension 768 for table BankruptcyAct\n",
      "Successfully added 597 records to BankruptcyAct\n",
      "✅ LanceDB index created/updated for 'BankruptcyAct'.\n",
      "\n",
      "⚙️ Indexing Company Act (Advanced Pipeline)...\n",
      "📄 Advanced processing for: CompaniesAct.pdf\n",
      "    Parsing with Unstructured (strategy: hi_res)... This may take a while.\n",
      "    Ensure Poppler is installed and in PATH for 'hi_res' PDF strategy if not using OCR-only methods.\n",
      "❌ Error during Unstructured parsing for CompaniesAct.pdf: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Dhanush.S\\\\AppData\\\\Local\\\\Temp\\\\tmpuutt4ipj\\\\8bc6da5c-d017-4db7-bd82-7dce712837fd-069.ppm'\n",
      "    Falling back to Fitz (PyMuPDF) for PDF text extraction.\n",
      "    Fitz (PyMuPDF) extracted text length: 1274481 chars.\n",
      "    Applying regex for coarse structuring...\n",
      "    Found 566 coarse chunks via regex.\n",
      "    Fine-graining long chunks...\n",
      "    Processed into 1080 final chunks for CompaniesAct.pdf.\n",
      "    Embedding 1080 chunks with main embedder...\n",
      "   LanceDB: Using embedding dimension 768 for table CompaniesAct\n",
      "Successfully added 1080 records to CompaniesAct\n",
      "✅ LanceDB index created/updated for 'CompaniesAct'.\n",
      "\n",
      "⚙️ Indexing Indian Constitution from CSV (Advanced Pipeline)...\n",
      "    Loaded 454 articles from CSV.\n",
      "    Fine-graining long articles...\n",
      "    Processed into 454 final chunks for Constitution.\n",
      "    Embedding 454 chunks with main embedder...\n",
      "   LanceDB: Using embedding dimension 768 for table IndianConstitution\n",
      "Successfully added 454 records to IndianConstitution\n",
      "✅ LanceDB index created/updated for 'IndianConstitution'.\n",
      "\n",
      "🎉 All advanced indexing pipelines completed.\n"
     ]
    }
   ],
   "source": [
    "# RUN ADVANCED INDEXING PIPELINE\n",
    "print(\"▶️ Initializing main embedder for LanceDB storage...\")\n",
    "\n",
    "# --- Choose your MAIN embedder for storage ---\n",
    "# # Option 1: Your existing LegalBERTEmbedder (ensure legalbert_embedder.py is present)\n",
    "# main_embedder = LegalBERTEmbedder() \n",
    "# print(f\"   Using LegalBERTEmbedder (model: {config['model_name']}) as the main embedder.\")\n",
    "\n",
    "# Option 2: A different SentenceTransformer model\n",
    "main_embedder_model_name = \"sentence-transformers/all-mpnet-base-v2\" # or \"thenlper/gte-base\"\n",
    "print(f\"   Using '{main_embedder_model_name}' as the main embedder.\")\n",
    "main_embedder = SentenceTransformer(main_embedder_model_name)\n",
    "# IMPORTANT: If you use Option 2 and the model has a different embedding dimension than 768,\n",
    "# you MUST update the Vector(dimension) in your LanceModel schemas in the create_lancedb_index_* functions.\n",
    "# For \"all-mpnet-base-v2\" or \"thenlper/gte-base\", the dimension is 768.\n",
    "\n",
    "config[\"main_embedder_model_name\"] = main_embedder_model_name\n",
    "\n",
    "index_bankruptcy_act_advanced(config, main_embedder)\n",
    "index_company_act_advanced(config, main_embedder)\n",
    "index_constitution_articles_advanced(config, main_embedder)\n",
    "\n",
    "print(\"\\n🎉 All advanced indexing pipelines completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retriever-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "# from legalbert_embedder import LegalBERTEmbedder # Already imported earlier if main_embedder is LegalBERTEmbedder\n",
    "# from sentence_transformers import SentenceTransformer # Already imported earlier if main_embedder is SentenceTransformer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "legal-retriever-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalRetriever:\n",
    "    def __init__(self, main_embedder_instance, top_k: int = 5):\n",
    "        self.model = main_embedder_instance # Use the same embedder instance used for indexing\n",
    "        self.top_k = top_k\n",
    "        self.dbs = {}\n",
    "\n",
    "    def _get_table(self, db_path: str, table_name: str):\n",
    "        db_key = f\"{db_path}_{table_name}\" # Unique key for db connection + table\n",
    "        if db_key not in self.dbs:\n",
    "            try:\n",
    "                db_conn = lancedb.connect(db_path)\n",
    "                self.dbs[db_key] = db_conn.open_table(table_name)\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Failed to connect to DB at {db_path} or open table {table_name}: {str(e)}\")\n",
    "        return self.dbs[db_key]\n",
    "\n",
    "    def query_multiple(self, query_text: str, tables_to_search: list[dict]) -> list:\n",
    "        try:\n",
    "            query_vec = self.model.encode([query_text])[0].tolist()\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to embed query: {str(e)}\")\n",
    "\n",
    "        all_results_dfs = []\n",
    "\n",
    "        for tbl_info in tables_to_search:\n",
    "            try:\n",
    "                table_obj = self._get_table(tbl_info[\"db_path\"], tbl_info[\"table_name\"])\n",
    "                df = table_obj.search(query_vec).limit(self.top_k).to_pandas()\n",
    "                all_results_dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to query table '{tbl_info['table_name']}' in DB '{tbl_info['db_path']}': {str(e)}\")\n",
    "        \n",
    "        if not all_results_dfs:\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            merged_df = pd.concat(all_results_dfs, ignore_index=True)\n",
    "            if \"_distance\" in merged_df.columns:\n",
    "                merged_df = merged_df.sort_values(by=\"_distance\", ascending=True)\n",
    "            else:\n",
    "                 print(\"Warning: '_distance' column not found in search results for sorting.\")\n",
    "        except Exception as e:\n",
    "            # If concat fails (e.g. empty list of dfs), return empty list or handle\n",
    "            if not all_results_dfs:\n",
    "                return []\n",
    "            raise RuntimeError(f\"Failed to process merged results: {str(e)}\")\n",
    "        \n",
    "        # Format results\n",
    "        output_results = []\n",
    "        for _, row in merged_df.head(self.top_k).iterrows(): # Ensure only top_k overall are returned\n",
    "            output_results.append({\n",
    "                \"id\": row.get(\"id\"),\n",
    "                \"chunk\": row.get(\"chunk\"),\n",
    "                \"part_title\": row.get(\"part_title\"),\n",
    "                \"chapter_title\": row.get(\"chapter_title\"),\n",
    "                \"section_title\": row.get(\"section_title\"),\n",
    "                \"page\": row.get(\"page\"),\n",
    "                \"source\": row.get(\"source\"),\n",
    "                \"score\": row.get(\"_distance\")\n",
    "            })\n",
    "        return output_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "query-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize retriever with the same main_embedder instance used for indexing\n",
    "# This ensures consistency in embeddings for query and storage.\n",
    "# 'main_embedder' should be defined from the indexing execution cell above.\n",
    "main_embedder_model_name = \"sentence-transformers/all-mpnet-base-v2\" # or \"thenlper/gte-base\"\n",
    "print(f\"   Using '{main_embedder_model_name}' as the main embedder.\")\n",
    "main_embedder = SentenceTransformer(main_embedder_model_name)\n",
    "\n",
    "retriever = LegalRetriever(main_embedder_instance=main_embedder, top_k=5)\n",
    "\n",
    "def query_legal_documents(query: str) -> List[Dict]:\n",
    "    tables_to_query = [\n",
    "        {\"db_path\": config[\"db_path\"], \"table_name\": config[\"comp_table\"]},\n",
    "        {\"db_path\": config[\"db_path\"], \"table_name\": config[\"bank_table\"]},\n",
    "        {\"db_path\": config[\"db_path\"], \"table_name\": config[\"constitution_table\"]}\n",
    "    ]\n",
    "    return retriever.query_multiple(query, tables_to_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query-example-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results1 = query_legal_documents(query = \"What are the requirements for corporate insolvency resolution under the Companies Act and the Insolvency and Bankruptcy Code?\")\n",
    "for res in results1:\n",
    "    print(f\"Source: {res['source']}, Section: {res.get('section_title', res.get('id'))}, Score: {res['score']:.2f}\")\n",
    "    print(f\"Chunk: {res['chunk'][:300]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query-example-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = query_legal_documents(query = \"Provided that a company in respect of which such appeal or reference or inquiry stands abated under this clause may make reference to the National Company Law Tribunal under the Insolvency and Bankruptcy Code, 2016 within one hundred and eighty days from the commencement of the Insolvency and Bankruptcy Code, 2016 in accordance with the provisions of the Insolvency and Bankruptcy Code, 2016:\")\n",
    "for res in results2:\n",
    "    print(f\"Source: {res['source']}, Section: {res.get('section_title', res.get('id'))}, Score: {res['score']:.2f}\")\n",
    "    print(f\"Chunk: {res['chunk'][:300]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "query-example-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Companies Act, Section: section 447, Score: 0.59\n",
      "Chunk: 58 \n",
      " \n",
      "68. Power of company to purchase its own securities.—(1) Notwithstanding anything contained in \n",
      "this Act, but subject to the provisions of sub-section (2), a company may purchase its own shares or other \n",
      "specified securities (hereinafter referred to as buy-back) out of— \n",
      "(a) its free reserves;...\n",
      "\n",
      "Source: Companies Act, Section: section 133, Score: 0.65\n",
      "Chunk: section 133 or any other provision of this Act and a certificate to that effect by the \n",
      "company’s auditor has been filed with the Tribunal. \n",
      "(4) The order of confirmation of the reduction of share capital by the Tribunal under sub-section (3) \n",
      "shall be published by the company in such manner as the ...\n",
      "\n",
      "Source: Companies Act, Section: section 62, Score: 0.66\n",
      "Chunk: section 62 or other specified securities within a period of six \n",
      "months except by way of a bonus issue or in the discharge of subsisting obligations such as conversion of \n",
      "warrants, stock option schemes, sweat equity or conversion of preference shares or debentures into equity \n",
      "shares. (9) Where a c...\n",
      "\n",
      "Source: Companies Act, Section: section 447, Score: 0.73\n",
      "Chunk: 67. Restriction on purchase by company or giving of loans by it for purchase of its shares.—(1) \n",
      "No company limited by shares or by guarantee and having a share capital shall have power to buy its own \n",
      "shares unless the consequent reduction of share capital is effected under the provisions of this A...\n",
      "\n",
      "Source: Companies Act, Section: section 70, Score: 0.74\n",
      "Chunk: 14 (w.e.f. 21-12-\n",
      "2020). 2. Subs. by s. 14, ibid., for “three lakh rupees, or with both” (w.e.f. 21-12-2020). 60 \n",
      " \n",
      "value of the shares so purchased shall be transferred to the capital redemption reserve account and details \n",
      "of such transfer shall be disclosed in the balance sheet. (2) The capital r...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results3 = query_legal_documents(query = \"What are the regulations on buying back shares under the Companies Act?\")\n",
    "for res in results3:\n",
    "    print(f\"Source: {res['source']}, Section: {res.get('section_title', res.get('id'))}, Score: {res['score']:.2f}\")\n",
    "    print(f\"Chunk: {res['chunk'][:300]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debug-fetch-by-id-arrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "import pyarrow.compute as pc\n",
    "\n",
    "# Example: Inspect a specific document from BankruptcyAct\n",
    "try:\n",
    "    db_inspect = lancedb.connect(config[\"db_path\"])\n",
    "    table_inspect_name = config[\"bank_table\"]\n",
    "    if db_inspect.table_exists(table_inspect_name):\n",
    "        table_inspect = db_inspect.open_table(table_inspect_name)\n",
    "        \n",
    "        # Fetch all IDs to find one for testing, or use a known one\n",
    "        all_data = table_inspect.to_arrow()\n",
    "        if len(all_data) > 0:\n",
    "            # Pick the first ID as an example, if IDs are structured like 'bankruptcy_section_0_sub_0'\n",
    "            doc_id_to_find = all_data['id'][0].as_py()\n",
    "            print(f\"Attempting to fetch document with ID: {doc_id_to_find} from {table_inspect_name}\")\n",
    "            \n",
    "            mask = pc.equal(all_data[\"id\"], doc_id_to_find)\n",
    "            filtered_table = all_data.filter(mask)\n",
    "            result = filtered_table.to_pylist()\n",
    "            \n",
    "            if result:\n",
    "                print(\"\\nDocument found:\")\n",
    "                print(result[0])\n",
    "                print(\"\\nKeys in document:\", result[0].keys())\n",
    "            else:\n",
    "                print(f\"Document with ID {doc_id_to_find} not found in {table_inspect_name}.\")\n",
    "        else:\n",
    "            print(f\"Table {table_inspect_name} is empty.\")\n",
    "    else:\n",
    "        print(f\"Table {table_inspect_name} does not exist in {config['db_path']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during debug inspection: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-final-cell",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
